{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd7ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "import json\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "import json\n",
    "from utils.db import get_table_columns  # <-- make sure this exists in your utils.db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4178e40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\A4627\\OneDrive - Axtria\\DSA_Python\\Agentic_DataIngestion\\utils\\db.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n",
      "c:\\Users\\A4627\\OneDrive - Axtria\\DSA_Python\\Agentic_DataIngestion\\utils\\db.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'target_column': 'brand_id', 'source_columns': ['brand_id'], 'transformation': None}, {'target_column': 'brand_name', 'source_columns': ['brand_name'], 'transformation': None}, {'target_column': 'is_active', 'source_columns': ['process_ind'], 'transformation': \"CASE WHEN process_ind = 'I' THEN 'Y' WHEN process_ind = 'U' THEN 'Y' ELSE 'N' END\"}, {'target_column': 'eff_start_dt', 'source_columns': ['audit_insrt_dt'], 'transformation': None}, {'target_column': 'eff_end_date', 'source_columns': [], 'transformation': 'CURRENT_TIMESTAMP'}, {'target_column': 'audit_insrt_dt', 'source_columns': ['audit_insrt_dt'], 'transformation': None}, {'target_column': 'audit_insrt_nm', 'source_columns': ['audit_insrt_nm'], 'transformation': None}, {'target_column': 'audit_updt_dt', 'source_columns': ['audit_updt_dt'], 'transformation': 'CURRENT_TIMESTAMP'}, {'target_column': 'audit_updt_nm', 'source_columns': ['audit_updt_nm'], 'transformation': None}]\n"
     ]
    }
   ],
   "source": [
    "src_schema = 'int'\n",
    "src_table = 'brand_master'\n",
    "tgt_schema = 'dwh'\n",
    "tgt_table = 'brand_master'\n",
    "\n",
    "# Fetch schema metadata from DB\n",
    "source_columns = get_table_columns(src_schema, src_table)\n",
    "target_columns = get_table_columns(tgt_schema, tgt_table)\n",
    "\n",
    "# Extract just the column names for prompt clarity\n",
    "src_cols = [c[\"column_name\"] for c in source_columns]\n",
    "tgt_cols = [c[\"column_name\"] for c in target_columns]\n",
    "\n",
    "# Build LLM model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.2,api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Strict instruction to use only target columns\n",
    "messages = [\n",
    "    SystemMessage(content=(\n",
    "        \"You are an expert ETL engineer. \"\n",
    "        \"Your task is to build a JSON Source-To-Target Mapping (STTM). \"\n",
    "        \"You must only use the target columns that exist in the provided schema metadata. \"\n",
    "        \"Do not invent or hallucinate new target columns. \"\n",
    "        \"You may, however, infer transformations or use constants for audit fields like timestamps.\"\n",
    "\n",
    "    )),\n",
    "    HumanMessage(content=f\"\"\"\n",
    "    Given:\n",
    "    - Source table: `{src_schema}.{src_table}`\n",
    "    - Target table: `{tgt_schema}.{tgt_table}`\n",
    "\n",
    "    Source columns:\n",
    "    {json.dumps(src_cols, indent=2)}\n",
    "\n",
    "    Target columns:\n",
    "    {json.dumps(tgt_cols, indent=2)}\n",
    "\n",
    "    Generate an STTM JSON with entries like this:\n",
    "    {{\n",
    "        \"target_column\": \"<column_name_in_target>\",\n",
    "        \"source_columns\": [\"col1\", \"col2\", ...],\n",
    "        \"transformation\": \"<expression or null>\"\n",
    "    }}\n",
    "\n",
    "    Rules:\n",
    "    - Map only columns present in the target list.\n",
    "    - Use similar names for direct mappings (case-insensitive) except fou audit columns.\n",
    "    - For audit fields like 'audit_insrt_dt', 'audit_updt_dt', 'load_ts', use CURRENT_TIMESTAMP.\n",
    "    - Output must be valid JSON (list of mappings).\n",
    "    -INT schema tables are always SCD1 for dimensions\n",
    "    -DWH Schema tables are always SCD2 for dimensions.\n",
    "    -Process_ind in the INT Schema will be I for new records & U for updated records & D for deleted records.\n",
    "    -When the source schema is INT and target schema is DWH pick up records which have processind in I and U.\n",
    "    -DWH tables will have is_active column which is a flag .Its set to Y for currently active records and N for inactive records.\n",
    "    -EFF_START_DT & EFF_END_DT are the dates in between which the records were active/currently active.\n",
    "\n",
    "    \"\"\")\n",
    "]\n",
    "\n",
    "response = model.invoke(messages)\n",
    "reply = response.content.strip()\n",
    "\n",
    "# Safe JSON extraction\n",
    "try:\n",
    "    sttm = json.loads(reply)\n",
    "except Exception:\n",
    "    import re\n",
    "    match = re.search(r\"\\[[\\s\\S]*\\]\", reply)\n",
    "    sttm = json.loads(match.group(0)) if match else []\n",
    "\n",
    "# Double-check target column validity (safety filter)\n",
    "valid_tgt_cols = set(tgt_cols)\n",
    "filtered_sttm = [m for m in sttm if m.get(\"target_column\") in valid_tgt_cols]\n",
    "print(filtered_sttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e08c17cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Given that both the source and target tables have no specified columns, the JSON Source-To-Target Mapping (STTM) will also be empty. However, I will provide a template for the STTM based on the rules and structure you provided, assuming that there would be columns in a typical scenario.\\n\\nHereâ€™s how the STTM JSON would look like if there were columns to map:\\n\\n```json\\n[]\\n```\\n\\nIf you had specific columns in mind for the source and target tables, please provide them, and I can create a more detailed mapping based on that information.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95f81cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03261b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\A4627\\OneDrive - Axtria\\DSA_Python\\Agentic_DataIngestion\\utils\\db.py:97: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'column_name': 'brand_id', 'data_type': 'text'},\n",
       " {'column_name': 'brand_name', 'data_type': 'text'},\n",
       " {'column_name': 'process_ind', 'data_type': 'text'},\n",
       " {'column_name': 'audit_insrt_dt', 'data_type': 'timestamp without time zone'},\n",
       " {'column_name': 'audit_insrt_nm', 'data_type': 'text'},\n",
       " {'column_name': 'audit_updt_dt', 'data_type': 'timestamp without time zone'},\n",
       " {'column_name': 'audit_updt_nm', 'data_type': 'text'},\n",
       " {'column_name': 'audit_batch_id', 'data_type': 'text'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=get_table_columns('int', '')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb9ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee5b154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb350b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15bb166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1b81a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
